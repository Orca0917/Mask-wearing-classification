{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(augmentation='CustomAugmentation', batch_size=32, criterion='cross_entropy', data_dir='/opt/ml/input/data/train/images', dataset='CutMixDataset', epochs=10, log_interval=20, lr=0.0001, lr_decay_step=5, model='EfficientNet_MultiLabel', model_dir='./model/', name='Face_Cutmix', optimizer='Adam', resize=[380, 380], seed=42, val_ratio=0.2, valid_batch_size=32)\n",
      "Loaded pretrained weights for efficientnet-b4\n",
      "Epoch[0/10](20/411) || training loss 2.782 || training accuracy 22.34% || lr 0.0001\n",
      "Epoch[0/10](40/411) || training loss 2.47 || training accuracy 42.50% || lr 0.0001\n",
      "Epoch[0/10](60/411) || training loss 1.921 || training accuracy 61.25% || lr 0.0001\n",
      "Epoch[0/10](80/411) || training loss 1.383 || training accuracy 66.56% || lr 0.0001\n",
      "Epoch[0/10](100/411) || training loss 0.9786 || training accuracy 78.59% || lr 0.0001\n",
      "Epoch[0/10](120/411) || training loss 0.7747 || training accuracy 80.62% || lr 0.0001\n",
      "Epoch[0/10](140/411) || training loss 0.5795 || training accuracy 86.72% || lr 0.0001\n",
      "Epoch[0/10](160/411) || training loss 0.5024 || training accuracy 86.88% || lr 0.0001\n",
      "Epoch[0/10](180/411) || training loss 0.4468 || training accuracy 87.50% || lr 0.0001\n",
      "Epoch[0/10](200/411) || training loss 0.4599 || training accuracy 86.41% || lr 0.0001\n",
      "Epoch[0/10](220/411) || training loss 0.3815 || training accuracy 88.75% || lr 0.0001\n",
      "Epoch[0/10](240/411) || training loss 0.3307 || training accuracy 90.47% || lr 0.0001\n",
      "Epoch[0/10](260/411) || training loss 0.3233 || training accuracy 90.78% || lr 0.0001\n",
      "Epoch[0/10](280/411) || training loss 0.2845 || training accuracy 91.56% || lr 0.0001\n",
      "Epoch[0/10](300/411) || training loss 0.293 || training accuracy 91.56% || lr 0.0001\n",
      "Epoch[0/10](320/411) || training loss 0.2853 || training accuracy 90.94% || lr 0.0001\n",
      "Epoch[0/10](340/411) || training loss 0.2371 || training accuracy 91.88% || lr 0.0001\n",
      "Epoch[0/10](360/411) || training loss 0.2271 || training accuracy 92.66% || lr 0.0001\n",
      "Epoch[0/10](380/411) || training loss 0.2126 || training accuracy 93.75% || lr 0.0001\n",
      "Epoch[0/10](400/411) || training loss 0.1805 || training accuracy 94.69% || lr 0.0001\n",
      "Calculating validation results...\n",
      "New best model for val accuracy : 86.58%! saving the best model..\n",
      "[Val] acc : 86.58%, loss: 0.37 || best acc : 86.58%, best loss: 0.37\n",
      "\n",
      "Epoch[1/10](20/411) || training loss 0.1833 || training accuracy 95.47% || lr 9.938441702975689e-05\n",
      "Epoch[1/10](40/411) || training loss 0.2113 || training accuracy 94.84% || lr 9.938441702975689e-05\n",
      "Epoch[1/10](60/411) || training loss 0.1912 || training accuracy 94.22% || lr 9.938441702975689e-05\n",
      "Epoch[1/10](80/411) || training loss 0.1546 || training accuracy 95.00% || lr 9.938441702975689e-05\n",
      "Epoch[1/10](100/411) || training loss 0.1333 || training accuracy 95.78% || lr 9.938441702975689e-05\n",
      "Epoch[1/10](120/411) || training loss 0.1553 || training accuracy 95.31% || lr 9.938441702975689e-05\n",
      "Epoch[1/10](140/411) || training loss 0.1795 || training accuracy 93.44% || lr 9.938441702975689e-05\n",
      "Epoch[1/10](160/411) || training loss 0.15 || training accuracy 96.09% || lr 9.938441702975689e-05\n",
      "Epoch[1/10](180/411) || training loss 0.1229 || training accuracy 97.19% || lr 9.938441702975689e-05\n",
      "Epoch[1/10](200/411) || training loss 0.1194 || training accuracy 96.25% || lr 9.938441702975689e-05\n",
      "Epoch[1/10](220/411) || training loss 0.1141 || training accuracy 97.19% || lr 9.938441702975689e-05\n",
      "Epoch[1/10](240/411) || training loss 0.1491 || training accuracy 95.78% || lr 9.938441702975689e-05\n",
      "Epoch[1/10](260/411) || training loss 0.09264 || training accuracy 97.19% || lr 9.938441702975689e-05\n",
      "Epoch[1/10](280/411) || training loss 0.07821 || training accuracy 98.12% || lr 9.938441702975689e-05\n",
      "Epoch[1/10](300/411) || training loss 0.09094 || training accuracy 97.97% || lr 9.938441702975689e-05\n",
      "Epoch[1/10](320/411) || training loss 0.1175 || training accuracy 96.72% || lr 9.938441702975689e-05\n",
      "Epoch[1/10](340/411) || training loss 0.0832 || training accuracy 98.12% || lr 9.938441702975689e-05\n",
      "Epoch[1/10](360/411) || training loss 0.09668 || training accuracy 97.03% || lr 9.938441702975689e-05\n",
      "Epoch[1/10](380/411) || training loss 0.09711 || training accuracy 96.88% || lr 9.938441702975689e-05\n",
      "Epoch[1/10](400/411) || training loss 0.08496 || training accuracy 97.66% || lr 9.938441702975689e-05\n",
      "Calculating validation results...\n",
      "New best model for val accuracy : 93.15%! saving the best model..\n",
      "[Val] acc : 93.15%, loss: 0.19 || best acc : 93.15%, best loss: 0.19\n",
      "\n",
      "Epoch[2/10](20/411) || training loss 0.09265 || training accuracy 97.81% || lr 9.755282581475769e-05\n",
      "Epoch[2/10](40/411) || training loss 0.08526 || training accuracy 97.97% || lr 9.755282581475769e-05\n",
      "Epoch[2/10](60/411) || training loss 0.09325 || training accuracy 97.50% || lr 9.755282581475769e-05\n",
      "Epoch[2/10](80/411) || training loss 0.07365 || training accuracy 97.81% || lr 9.755282581475769e-05\n",
      "Epoch[2/10](100/411) || training loss 0.07152 || training accuracy 97.81% || lr 9.755282581475769e-05\n"
     ]
    }
   ],
   "source": [
    "!python /opt/ml/level1-image-classification-level1-recsys-04/model/FaceAndCutmix/train.py \\\n",
    "    --epochs 10 \\\n",
    "    --dataset 'CutMixDataset' \\\n",
    "    --augmentation 'CustomAugmentation' \\\n",
    "    --batch_size 32 \\\n",
    "    --valid_batch_size 32 \\\n",
    "    --model 'EfficientNet_MultiLabel' \\\n",
    "    --lr '1e-4' \\\n",
    "    --lr_decay_step '5' \\\n",
    "    --name 'Face_Cutmix' \\\n",
    "    --model_dir './model/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n",
      "Calculating inference results..\n",
      "Inference Done!\n"
     ]
    }
   ],
   "source": [
    "!python /opt/ml/level1-image-classification-level1-recsys-04/model/FaceAndCutmix/inference.py \\\n",
    "    --batch_size 32 \\\n",
    "    --model 'EfficientNet_MultiLabel' \\\n",
    "    --augmentation 'BaseAugmentation' \\\n",
    "    --model_dir './model/Face_Cutmix/' \\\n",
    "    --output_dir './output/Face_Cutmix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
